{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ea7cd9ae",
   "metadata": {},
   "source": [
    "<h1 style=\"font-family: serif\">Exercise: Transformer Representation</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75b13bf7",
   "metadata": {},
   "source": [
    "In this exercise, we'll learn how to extract features out of a pre-trained transformer model and use those features for downstream tasks. For this exercise, we'll use [*DINOv2: A Self-supervised Vision Transformer Model*](https://dinov2.metademolab.com/) by *meta*. This model is trained in a teacher-student paradigm, without any supervision, and it produces features suitable for different downstream tasks like image classification, depth estimation, semantic segmentation, etc.\n",
    "<br><br>**Note:** DINOv2 makes 14x14 patches out of an input image, and then produce features for each patch (not for each pixel)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dc580c5",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-danger\">\n",
    "    Please switch to the <code>05_learned_representations</code> environment.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "410618e0",
   "metadata": {},
   "source": [
    "<h2 style=\"font-family: serif\">I. Part One</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb567551",
   "metadata": {},
   "source": [
    "In the first part, we will examine and visualize the extracted features using **PCA** and **UMAP**. At the end of this part, we use **KMeans** on top of the extracted features to cluster them, and to compare obtained clusters with given ground truth masks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b691346",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torchvision.transforms import v2 as tv_transforms2\n",
    "\n",
    "import umap\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "import utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ed735b5",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "%matplotlib widget\n",
    "plt.ioff()\n",
    "\n",
    "SEED = 2024\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "torch.cuda.manual_seed(SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5926f0a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# color map for visualization of the ground truth masks\n",
    "cm, colors = utils.get_colormap()\n",
    "cm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3743b84b",
   "metadata": {},
   "source": [
    "## Data\n",
    "We are using data provided by [*Dense cellular segmentation for EM using 2D-3D neural network ensembles*](https://leapmanlab.github.io/dense-cell/).\n",
    "<br>The data contains *tiff* files in train and evaluation sets along with their ground truth masks. Masks include dense annotations for seven classes.\n",
    "<br>Images have a resolution of 800 x 800 pixels which are a bit large to fit in memory or GPU üòÅ . However, we'll transform them into a smaller manageable resolution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c74abf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# the data resides in this path: \"/group/dl4miacourse/platelet_data\"\n",
    "# load data and ground truth masks\n",
    "data_images = utils.get_images_from_tiff(\n",
    "    \"/group/dl4miacourse/platelet_data/train-images.tif\", to_rgb=True\n",
    ")\n",
    "gt_masks = utils.get_images_from_tiff(\n",
    "    \"/group/dl4miacourse/platelet_data/train-labels.tif\", to_rgb=False\n",
    ")\n",
    "\n",
    "print(data_images.shape, gt_masks.shape)\n",
    "utils.plot_data_sample(data_images[0], gt_masks[0], cmap=cm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b59a95b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# original image size\n",
    "image_size = data_images.shape[1]\n",
    "# ground truth classes and their labels\n",
    "num_classes = 7\n",
    "classes = {\n",
    "    \"background\": 0,\n",
    "    \"cell\": 1,\n",
    "    \"mitochondrion\": 2,\n",
    "    \"alpha granule\": 3,\n",
    "    \"canalicular channel\": 4,\n",
    "    \"dense granule\": 5,\n",
    "    \"dense granule core\": 6\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0024a906",
   "metadata": {},
   "source": [
    "## Load the Pre-trained Transformer Model\n",
    "We use pre-trained DINOv2 small model for feature extraction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94b7a589",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e26e83de",
   "metadata": {},
   "outputs": [],
   "source": [
    "dinov2 = torch.hub.load(\"facebookresearch/dinov2\", \"dinov2_vits14_reg\").to(device)\n",
    "dinov2.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df0da281",
   "metadata": {},
   "source": [
    "We will use the dino's `get_intermediate_layers` method to extract features from the DINOv2 model.  \n",
    "- The first parameter is an input image batch. \n",
    "- The second parameter, `n`, points to model's layer(s) to extract features from (layers or n last layers to take).  \n",
    "- If `reshape=True`, the features will be returned as a batch of 3D : (F-size, W, H), else it will be 2D ((W x H), F-size).  \n",
    "- We don't want the class token, so `return_class_token=False`.  \n",
    "<br><br>\n",
    "This method returns a tuple of features with each element points to a requested layer.\n",
    "<br> See the code [*here*](https://github.com/facebookresearch/dinov2/blob/e1277af2ba9496fbadf7aec6eba56e8d882d1e35/dinov2/models/vision_transformer.py#L298)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "107e8c40",
   "metadata": {},
   "outputs": [],
   "source": [
    "help(dinov2.get_intermediate_layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67006820",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"torch.cuda.memory_allocated: %fGB\"%(torch.cuda.memory_allocated(0)/1024/1024/1024))\n",
    "print(\"torch.cuda.memory_reserved: %fGB\"%(torch.cuda.memory_reserved(0)/1024/1024/1024))\n",
    "print(\"torch.cuda.max_memory_reserved: %fGB\"%(torch.cuda.max_memory_reserved(0)/1024/1024/1024))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "073d9fbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DINOv2 trained on image patches of size 14 x 14. Therefore, the input image size should be divisible by 14. \n",
    "# dinov2_vits14_reg specs:\n",
    "feature_dim = 384\n",
    "patch_size = 14\n",
    "# to reduce original image resolution to integer number of patches\n",
    "num_patches = 30\n",
    "\n",
    "input_size = patch_size * num_patches\n",
    "print(f\"Dino input image size: {input_size}\")\n",
    "\n",
    "# define proper image/mask transformation\n",
    "dino_transforms = tv_transforms2.Compose([\n",
    "    tv_transforms2.ToImage(),\n",
    "    tv_transforms2.Resize(input_size, interpolation=tv_transforms2.InterpolationMode.BILINEAR),\n",
    "    tv_transforms2.ToDtype(dtype=torch.float32, scale=True),\n",
    "])\n",
    "\n",
    "mask_transforms = tv_transforms2.Compose([\n",
    "    tv_transforms2.ToImage(),\n",
    "    tv_transforms2.Resize(input_size, interpolation=tv_transforms2.InterpolationMode.NEAREST)\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3aa82dce",
   "metadata": {},
   "source": [
    "## Feature Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "503f87b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# select a random batch of images and their masks\n",
    "batch_size = 12\n",
    "random_indices = torch.randperm(len(data_images))\n",
    "image_batch = data_images[random_indices[:batch_size]]\n",
    "mask_batch = gt_masks[random_indices[:batch_size]]\n",
    "\n",
    "print(image_batch.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5d7bf21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# transform the batch for the dino model,\n",
    "# also, we downscale the gt masks to the input size.\n",
    "transformed_images = []\n",
    "transformed_masks = []\n",
    "\n",
    "for i in range(len(image_batch)):\n",
    "    transformed_images.append(dino_transforms(image_batch[i]))\n",
    "    transformed_masks.append(mask_transforms(mask_batch[i][:, :, np.newaxis]))\n",
    "\n",
    "transformed_images = torch.stack(transformed_images)\n",
    "transformed_masks = torch.stack(transformed_masks).squeeze(1)\n",
    "\n",
    "print(transformed_images.shape, transformed_masks.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14059e28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract the features\n",
    "with torch.no_grad():\n",
    "    for i in range(len(image_batch)):\n",
    "        features = dinov2.get_intermediate_layers(\n",
    "            transformed_images,\n",
    "            n=1,\n",
    "            return_class_token=False,\n",
    "            reshape=False,\n",
    "            norm=True\n",
    "        )[0]\n",
    "\n",
    "print(features.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0420204d",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\">\n",
    "  <h3>Checkpoint 1</h3>\n",
    "  <p>At this point we got familiar with the data, and the DINOv2 model loading and feature extraction process.</p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d1529b8",
   "metadata": {},
   "source": [
    "## Visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abe15691",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\">\n",
    "  <h3>Task 1.1: PCA on the extracted features</h3>\n",
    "  <p>\n",
    "      We want to use <i>PCA</i> as a dimensionality reduction algorithm to get first <i>3</i> principal components.<br>Then plot the outcome to compare reduced feature space with the pixel space, using those PCA components as RGB channels.\n",
    "  </p>\n",
    "<p><i>\n",
    "   Please refer to <a href=\"https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.PCA.html#sklearn.decomposition.PCA\">scikit-learn <b>PCA</b> documentation</a>\n",
    "</i>.</p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "715d325f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# flatten the features across all image patches (30x30)\n",
    "flatten_features = features.cpu().numpy().reshape((-1, feature_dim))\n",
    "print(flatten_features.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85d9410b",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "# create low-res mask (30x30) to get approximate labels for each patch.\n",
    "low_res_masks = F.interpolate(\n",
    "    transformed_masks.unsqueeze(1),\n",
    "    size=(num_patches, num_patches),\n",
    "    mode=\"nearest-exact\"\n",
    ").squeeze(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4291f77c",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "# provided function for plotting\n",
    "def plot_pca(image, pca_image):\n",
    "    if image.shape[0] == 3:\n",
    "        image = image[0]\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(6.5, 3), layout=\"compressed\")\n",
    "    fig.canvas.toolbar_position = \"right\"\n",
    "    fig.canvas.header_visible = False\n",
    "    fig.canvas.footer_visible = False\n",
    "\n",
    "    axes[0].imshow(image, cmap=\"grey\", origin=\"lower\")\n",
    "    axes[0].set_title(\"Image\")\n",
    "    axes[1].imshow(pca_image, origin=\"lower\")\n",
    "    axes[1].set_title(\"PCA\")\n",
    "\n",
    "    for ax in axes.ravel():\n",
    "        ax.set_aspect(\"equal\", \"box\")\n",
    "        # ax.set_axis_off()\n",
    "        ax.set_yticks([])\n",
    "        ax.xaxis.set_tick_params(labelsize=8)\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85348e14",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# get PCA first three components. use flatten_features as input.\n",
    "\n",
    "# insert your code here\n",
    "# pca = ...\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58f9f49b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# scale components into range of [0, 1]\n",
    "# insert your code here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5809ae0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# now reshape the acquired components to a batch of (num_patches x num_patches) RGB images\n",
    "# insert your code here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "507aa673",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# plot some samples using plot_pca() function.\n",
    "# use transformed_images as pixel images versus PCA images.\n",
    "# insert your code here\n",
    "# plot_pca(...)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d33e79a4",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\">\n",
    "  <h3>Task 1.2: UMAP on the extracted features</h3>\n",
    "  <p>\n",
    "      Now, we want to reduce the dimensionality of the extracted features, and plot the reduced features using <i>UMAP</i>.\n",
    "  </p>\n",
    "<p><i>\n",
    "   Please find the documents here: <a href=\"https://umap-learn.readthedocs.io/en/latest/parameters.html\"><b>UMAP</b></a>\n",
    "</i>.</p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a000e9db",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# insert your code here\n",
    "# reducer = ...\n",
    "# umap_embeddings = ...\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2724d25c",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "# plot UMAP\n",
    "fig, ax = plt.subplots(1, 1, figsize=(7, 6), layout=\"compressed\")\n",
    "fig.canvas.toolbar_position = \"right\"\n",
    "fig.canvas.header_visible = False\n",
    "fig.canvas.footer_visible = False\n",
    "\n",
    "labels = low_res_masks.numpy().flatten()\n",
    "\n",
    "ax.scatter(\n",
    "    umap_embeddings[:, 0],\n",
    "    umap_embeddings[:, 1],\n",
    "    s=10, c=labels, cmap=cm, alpha=0.5, lw=0\n",
    ")\n",
    "ax.set_xlabel(\"UMAP 1\")\n",
    "ax.set_ylabel(\"UMAP 2\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fb9c396",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\">\n",
    "  <h3>Checkpoint 2</h3>\n",
    "  <p>So far, we tried PCA and UMAP to reduce dimensionality of the extracted features for visualizing purposes.<br>As we can see, those reduced features can carry some information about the data classes and make a visually interesting representation, even though they have a low resolution.</p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7ab879b",
   "metadata": {},
   "source": [
    "## Clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7c1779b",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\">\n",
    "  <h3>Task 2.1: KMeans Clustering</h3>\n",
    "  <p>\n",
    "      Finally, we want to run a KMeans clustering on the extracted features to see how an unsupervised method can perform on separating the data classes.\n",
    "In other words, we want to find out if these features contain some information about the class they belong to.\n",
    "  </p>\n",
    "<p><i>\n",
    "   You can check out <b>KMeans</b> documentation <a href=\"https://scikit-learn.org/stable/modules/generated/sklearn.cluster.KMeans.html\">here</a>\n",
    "</i>.</p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d49f46ad",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "# clustering plot function\n",
    "def plot_clustering(image, gt, gt_low, pred, cmap=\"Dark2\", n_classes=7, clustering=\"KMeans\"):\n",
    "    if image.shape[0] == 3:\n",
    "        image = image[0]\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(7, 5.9), layout=\"compressed\")\n",
    "    fig.canvas.toolbar_position = \"right\"\n",
    "    fig.canvas.header_visible = False\n",
    "    fig.canvas.footer_visible = False\n",
    "    axes[0, 0].imshow(image, cmap=\"grey\", origin=\"lower\")\n",
    "    axes[0, 0].set_title(\"Image\")\n",
    "    axes[0, 1].imshow(gt, cmap=cmap, vmax=n_classes - 1, interpolation=\"none\", origin=\"lower\")\n",
    "    axes[0, 1].set_title(\"GT\")\n",
    "    axes[1, 0].imshow(gt_low, cmap=cmap, vmax=n_classes - 1, interpolation=\"none\", origin=\"lower\")\n",
    "    axes[1, 0].set_title(\"GT (low res.)\", y=-0.1, pad=2)\n",
    "    axes[1, 1].imshow(pred, cmap=\"Set2\", interpolation=\"none\", origin=\"lower\")\n",
    "    axes[1, 1].set_title(clustering, y=-0.1, pad=0)\n",
    "\n",
    "    for ax in axes.ravel():\n",
    "        ax.set_aspect(\"equal\", \"box\")\n",
    "        # ax.set_axis_off()\n",
    "        ax.set_xticks([])\n",
    "        ax.yaxis.set_tick_params(labelsize=8)\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "229beedd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# applying KMeans: use flatten_features as input.\n",
    "# set number of clusters as the same number of classes.\n",
    "\n",
    "# insert your code here\n",
    "# kmeans = ...\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53aea6b5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# get the predictions, and un-flatten it considering the batch_size.\n",
    "\n",
    "# insert your code here\n",
    "# predictions = ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbfc2f0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plotting some samples\n",
    "# select a sample from the batch and make it 2D\n",
    "selected_idx = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f093f36e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# insert your code here\n",
    "# pred_img = ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f4a8743",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_clustering(\n",
    "    image=transformed_images[selected_idx], gt=transformed_masks[selected_idx],\n",
    "    gt_low=low_res_masks[selected_idx], pred=pred_img,\n",
    "    cmap=cm, n_classes=num_classes\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7b4ebe4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# plot another sample\n",
    "\n",
    "# insert your code here\n",
    "# pred_img = ...\n",
    "\n",
    "# plot"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56d3b7be",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\">\n",
    "  <h3>Task 2.2: KMeans with different number of clusters</h3>\n",
    "  <p>\n",
    "      Try KMeans with different number of clusters and plot the results. See how it performs compares to semantic classes in the pixel space.\n",
    "  </p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce6e8316",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# insert your code here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bfd648c",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_img = predictions[0].reshape(num_patches, num_patches)\n",
    "plot_clustering(\n",
    "    transformed_images[0], transformed_masks[0],\n",
    "    low_res_masks[0], pred_img,\n",
    "    cm, num_classes\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7811beb6",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\">\n",
    "  <h3>Checkpoint 3</h3>\n",
    "  <p>We managed to run KMeans clustering on the extracted features and visualize the resulting clusters.\n",
    "</p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d8bfa6c",
   "metadata": {},
   "source": [
    "#### Optional Task\n",
    "Also, as an extra optional step, you may want to use different layers of the DINO model to extract features from,\n",
    "and see the differences in PCA or Clustering results.\n",
    "You can use `dinov2.get_intermediate_layers()` function and pass a list of layers indices or a single integer (check the feature extraction cell)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c311fd5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "tags,-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all",
   "text_representation": {
    "extension": ".py",
    "format_name": "percent"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
